Question/Answer Dataset, Release 1.2
====================================

This is the README file for the Question/Answer dataset generated by students 
who took undergraduate natural language processing courses taught by Noah Smith 
at Carnegie Mellon and Rebecca Hwa at the University of Pittsburgh during 
Spring 2008, Spring 2009, and Spring 2010.

There are three directories, one for each year of students: S08, S09, and S10.

The file "question_answer_pairs.txt" contains the questions and answers. The first line of the file contains 
column names for the tab-separated data fields in the file. This first line follows:

ArticleTitle    Question        Answer  DifficultyFromQuestioner        DifficultyFromAnswerer  ArticleFile

Field 1 is the name of the Wikipedia article from which questions and answers initially came.
Field 2 is the question.
Field 3 is the answer.
Field 4 is the prescribed difficulty rating for the question as given to the question-writer. 
Field 5 is a difficulty rating assigned by the individual who evaluated and answered the question, 
which may differ from the difficulty in field 4.
Field 6 is the relative path to the prefix of the article files. html files (.htm) and cleaned 
text (.txt) files are provided.

Questions that were judged to be poor were discarded from this data set.

There are frequently multiple lines with the same question, which appear if those questions were answered 
by multiple individuals. 

This particular release was prepared by Kevin Gimpel, but the data collection process 
was performed by Noah Smith, Mike Heilman, Rebecca Hwa, Shay Cohen, and many CMU students 
and Pitt students.

License Information:

The contents of S08 and S09 are released under the GFDL (http://www.gnu.org/licenses/fdl.html) and the contents of S10 are released under the CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0/). A copy of the GFDL license is included in the file named LICENSE-S08,S09. The reason for the different licenses is because Wikipedia moved from the GFDL to the CC BY-SA 3.0 license in the summer of 2009..

Release history:

2/18/2010: Version 1.0
  * Partial set of question/answer pairs from 2008 and 2009

8/6/2010: Version 1.1
  * All question/answer pairs from 2008, 2009, and 2010

8/23/2013: Version 1.2
  * Same data as Version 1.1, but now released under standard licenses

If you use this data in a publication, please cite the following:

Noah A. Smith, Michael Heilman, and Rebecca Hwa
Question Generation as a Competitive Undergraduate Course Project
In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA, September 2008. 
Available at: http://www.cs.cmu.edu/~nasmith/papers/smith+heilman+hwa.nsf08.pdf

Kevin Gimpel
kgimpel@cs.cmu.edu
kgimpel@ttic.edu
8/23/2013


Model Architecture

    Seq2Seq(
      (encoder): Encoder(
        (embedding): Embedding(2197, 256)
        (rnn): GRU(256, 512, bidirectional=True)
        (fc): Linear(in_features=1024, out_features=512, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (decoder): Decoder(
        (attention): Attention(
          (attn): Linear(in_features=1536, out_features=512, bias=True)
          (v): Linear(in_features=512, out_features=1, bias=False)
        )
        (embedding): Embedding(1504, 256)
        (rnn): GRU(1280, 512)
        (fc_out): Linear(in_features=1792, out_features=1504, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )


# [Model1](01_Question_Answer_Dataset_Sequence_to_Sequence_using_Attention.ipynb)


    Epoch: 01 | Time: 0m 8s
     Train Loss: 4.988 | Train PPL: 146.642
      Val. Loss: 3.802 |  Val. PPL:  44.788
    Epoch: 02 | Time: 0m 8s
     Train Loss: 4.286 | Train PPL:  72.709
      Val. Loss: 3.696 |  Val. PPL:  40.294
    Epoch: 03 | Time: 0m 8s
     Train Loss: 4.115 | Train PPL:  61.235
      Val. Loss: 3.700 |  Val. PPL:  40.465
    Epoch: 04 | Time: 0m 8s
     Train Loss: 3.936 | Train PPL:  51.189
      Val. Loss: 3.618 |  Val. PPL:  37.270
    Epoch: 05 | Time: 0m 8s
     Train Loss: 3.675 | Train PPL:  39.443
      Val. Loss: 3.593 |  Val. PPL:  36.357
    Epoch: 06 | Time: 0m 8s
     Train Loss: 3.395 | Train PPL:  29.824
      Val. Loss: 3.558 |  Val. PPL:  35.098
    Epoch: 07 | Time: 0m 8s
     Train Loss: 3.094 | Train PPL:  22.058
      Val. Loss: 3.543 |  Val. PPL:  34.575
    Epoch: 08 | Time: 0m 8s
     Train Loss: 2.770 | Train PPL:  15.961
      Val. Loss: 3.538 |  Val. PPL:  34.403
    Epoch: 09 | Time: 0m 8s
     Train Loss: 2.453 | Train PPL:  11.628
      Val. Loss: 3.541 |  Val. PPL:  34.486
    Epoch: 10 | Time: 0m 8s
     Train Loss: 2.214 | Train PPL:   9.152
      Val. Loss: 3.581 |  Val. PPL:  35.895

Test Loss

     | Test Loss: 3.439 | Test PPL:  31.145 |
       
    
# [Model2](02_Question_Answer_Seq2Seq_using_Attention_with_packed_padded_sequence_and_masking.ipynb)

Used packed padded sequences and masking - to the model from the previous notebook. Packed padded sequences are used to tell our RNN to skip over padding tokens in our encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements.
   
Training Log

    Epoch: 01 | Time: 0m 7s
     Train Loss: 5.200 | Train PPL: 181.184
     Val. Loss: 3.921 |  Val. PPL:  50.440
    Epoch: 02 | Time: 0m 7s
     Train Loss: 4.331 | Train PPL:  75.987
      Val. Loss: 3.726 |  Val. PPL:  41.507
    Epoch: 03 | Time: 0m 7s
     Train Loss: 4.099 | Train PPL:  60.276
      Val. Loss: 3.727 |  Val. PPL:  41.558
    Epoch: 04 | Time: 0m 7s
     Train Loss: 3.887 | Train PPL:  48.765
      Val. Loss: 3.663 |  Val. PPL:  38.987
    Epoch: 05 | Time: 0m 7s
     Train Loss: 3.671 | Train PPL:  39.278
      Val. Loss: 3.580 |  Val. PPL:  35.877
    Epoch: 06 | Time: 0m 7s
     Train Loss: 3.413 | Train PPL:  30.355
      Val. Loss: 3.509 |  Val. PPL:  33.425
    Epoch: 07 | Time: 0m 7s
     Train Loss: 3.171 | Train PPL:  23.834
      Val. Loss: 3.536 |  Val. PPL:  34.336
    Epoch: 08 | Time: 0m 7s
     Train Loss: 2.901 | Train PPL:  18.193
      Val. Loss: 3.524 |  Val. PPL:  33.914
    Epoch: 09 | Time: 0m 7s
     Train Loss: 2.619 | Train PPL:  13.722
      Val. Loss: 3.506 |  Val. PPL:  33.320
    Epoch: 10 | Time: 0m 7s
     Train Loss: 2.333 | Train PPL:  10.309
      Val. Loss: 3.523 |  Val. PPL:  33.884

Test Loss
    
    | Test Loss: 3.399 | Test PPL:  29.946 |

