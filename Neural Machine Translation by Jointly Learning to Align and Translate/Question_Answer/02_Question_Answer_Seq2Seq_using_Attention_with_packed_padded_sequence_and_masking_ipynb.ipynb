{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02-Question_Answer_Seq2Seq_using_Attention_with_packed padded sequence-and-masking.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divya-r-kamat/END/blob/main/Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate/Question_Answer/02_Question_Answer_Seq2Seq_using_Attention_with_packed_padded_sequence_and_masking_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C-PE8R83XdV"
      },
      "source": [
        "# Neural Machine Translation by Jointly Learning to Align and Translate using Packed Padded Sequences, Masking and BLEU\r\n",
        "\r\n",
        "\r\n",
        "In this notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). \r\n",
        "## Introduction\r\n",
        "\r\n",
        "As a reminder, here is the attention model:\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\r\n",
        "\r\n",
        "\r\n",
        "## Introduction\r\n",
        "\r\n",
        "In this notebook we will be adding a few improvements - packed padded sequences and masking - to the model from the previous notebook. Packed padded sequences are used to tell our RNN to skip over padding tokens in our encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements. Both of these techniques are commonly used in NLP. \r\n",
        "\r\n",
        "We will also look at how to use our model for inference, by giving it a sentence, seeing what it translates it as and seeing where exactly it pays attention to when translating each word.\r\n",
        "\r\n",
        "Finally, we'll use the BLEU metric to measure the quality of our translations.\r\n",
        "\r\n",
        "## Preparing Data\r\n",
        "\r\n",
        "Again, the preparation is similar to last time.\r\n",
        "First, we'll import all the modules as before, with the addition of the `matplotlib` modules used for viewing the attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1lNWCC-_rjg",
        "outputId": "a0145301-1794-4c70-f929-7063a7f9b527"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_AF5XWyDlRL",
        "outputId": "49e884bd-06fb-4201-bdc2-367b694e966f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_cnrcYwdX-A",
        "outputId": "57b81251-6fb5-423c-8f54-19b5ad463308"
      },
      "source": [
        "!ls /content/drive/MyDrive/Sequence-to-Sequence-using-Encoder-Decoder-and-Attention/Question_Answer_Seq_2_Seq"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data\n",
            "'Question_Answer_Dataset - Learning_Phrase_Representation_RNN Encoder-Decoder.ipynb'\n",
            "'Question_Answer_Dataset - Sequence_to_Sequence_using_Attention.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7CXKYZ0fLaM",
        "outputId": "b39841d6-64a2-4b54-b01d-80a760fcca77"
      },
      "source": [
        "!tar -xvf /content/drive/MyDrive/Sequence-to-Sequence-using-Encoder-Decoder-and-Attention/Question_Answer_Seq_2_Seq/data/Question_Answer_Dataset_v1.2.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question_Answer_Dataset_v1.2/\n",
            "Question_Answer_Dataset_v1.2/S08/\n",
            "Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/\n",
            "Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/LICENSE-S08,S09\n",
            "Question_Answer_Dataset_v1.2/README.v1.2\n",
            "Question_Answer_Dataset_v1.2/S09/\n",
            "Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3o.htm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysfsMhnWfQWs",
        "outputId": "d9a88f7a-af6a-46c1-b1bb-e258d948e991"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  Question_Answer_Dataset_v1.2  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNhEtnaZfTAk"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c125yPZO8uJK"
      },
      "source": [
        "There are three directories, one for each year of students: S08, S09, and S10. The file \"question_answer_pairs.txt\" contains the questions and answers. The first line of the file contains \r\n",
        "column names for the tab-separated data fields in the file. This first line follows:\r\n",
        "\r\n",
        "We will extract the question and answers from all the three files and concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGE6as9s23FW",
        "outputId": "5a96cc8d-2777-4072-cb01-64281bf966f0"
      },
      "source": [
        "df1 = pd.read_csv(\"./Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\",sep=\"\\t\",engine='python')\r\n",
        "df2 = pd.read_csv(\"./Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\",sep=\"\\t\",engine='python')\r\n",
        "df3 = pd.read_csv(\"./Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\",sep=\"\\t\",engine='python',quotechar='\"', error_bad_lines=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 765: '\t' expected after '\"'\n",
            "Skipping line 876: '\t' expected after '\"'\n",
            "Skipping line 1219: '\t' expected after '\"'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMz8avsr3-EB"
      },
      "source": [
        "df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "7xHeurgBfqdY",
        "outputId": "9f0cd1fc-0274-47a3-94b5-9917f6b48274"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did his mother die of pneumonia?</td>\n",
              "      <td>no</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ArticleTitle  ...   ArticleFile\n",
              "0  Abraham_Lincoln  ...  data/set3/a4\n",
              "1  Abraham_Lincoln  ...  data/set3/a4\n",
              "2  Abraham_Lincoln  ...  data/set3/a4\n",
              "3  Abraham_Lincoln  ...  data/set3/a4\n",
              "4  Abraham_Lincoln  ...  data/set3/a4\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSYcUJQawJ7D",
        "outputId": "a7fa8e01-f5ef-444c-9b0f-9ec545b11c75"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3995 entries, 0 to 3994\n",
            "Data columns (total 6 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   ArticleTitle              3995 non-null   object\n",
            " 1   Question                  3958 non-null   object\n",
            " 2   Answer                    3419 non-null   object\n",
            " 3   DifficultyFromQuestioner  3041 non-null   object\n",
            " 4   DifficultyFromAnswerer    3415 non-null   object\n",
            " 5   ArticleFile               3993 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 187.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqoiwZwk9Uya"
      },
      "source": [
        "Drop the rows which contains nulls values in Question and Answer columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "MlnGJCpxvr0T",
        "outputId": "1062df58-0f2a-4dda-b429-65d3a9a60556"
      },
      "source": [
        "df.dropna(subset=['Question','Answer'], inplace=True)\r\n",
        "df.reset_index(drop=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did his mother die of pneumonia?</td>\n",
              "      <td>no</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3414</th>\n",
              "      <td>Zebra</td>\n",
              "      <td>What areas do the Grevy's Zebras inhabit?</td>\n",
              "      <td>semi-arid grasslands of Ethiopia and northern ...</td>\n",
              "      <td>hard</td>\n",
              "      <td>hard</td>\n",
              "      <td>data/set1/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3415</th>\n",
              "      <td>Zebra</td>\n",
              "      <td>Which species of zebra is known as the common ...</td>\n",
              "      <td>Plains Zebra (Equus quagga, formerly Equus bur...</td>\n",
              "      <td>hard</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set1/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3416</th>\n",
              "      <td>Zebra</td>\n",
              "      <td>Which species of zebra is known as the common ...</td>\n",
              "      <td>Plains Zebra</td>\n",
              "      <td>hard</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set1/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3417</th>\n",
              "      <td>Zebra</td>\n",
              "      <td>At what age can a zebra breed?</td>\n",
              "      <td>five or six</td>\n",
              "      <td>hard</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set1/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3418</th>\n",
              "      <td>Zebra</td>\n",
              "      <td>At what age can a zebra breed?</td>\n",
              "      <td>5 or 6</td>\n",
              "      <td>hard</td>\n",
              "      <td>hard</td>\n",
              "      <td>data/set1/a9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3419 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         ArticleTitle  ...   ArticleFile\n",
              "0     Abraham_Lincoln  ...  data/set3/a4\n",
              "1     Abraham_Lincoln  ...  data/set3/a4\n",
              "2     Abraham_Lincoln  ...  data/set3/a4\n",
              "3     Abraham_Lincoln  ...  data/set3/a4\n",
              "4     Abraham_Lincoln  ...  data/set3/a4\n",
              "...               ...  ...           ...\n",
              "3414            Zebra  ...  data/set1/a9\n",
              "3415            Zebra  ...  data/set1/a9\n",
              "3416            Zebra  ...  data/set1/a9\n",
              "3417            Zebra  ...  data/set1/a9\n",
              "3418            Zebra  ...  data/set1/a9\n",
              "\n",
              "[3419 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXN0QPGzz0N4",
        "outputId": "6f8ab056-c491-4a90-e047-f8d3dc0d1921"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3419 entries, 0 to 3994\n",
            "Data columns (total 6 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   ArticleTitle              3419 non-null   object\n",
            " 1   Question                  3419 non-null   object\n",
            " 2   Answer                    3419 non-null   object\n",
            " 3   DifficultyFromQuestioner  2732 non-null   object\n",
            " 4   DifficultyFromAnswerer    3414 non-null   object\n",
            " 5   ArticleFile               3417 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 187.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_xZXxJfvoS"
      },
      "source": [
        "# Import Library\r\n",
        "import random\r\n",
        "import torch, torchtext\r\n",
        "from torchtext import data \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLrxIDu8gH4n"
      },
      "source": [
        "#Then set a random seed for deterministic results/reproducability.\r\n",
        "SEED = 1234\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqiCwLi9ziA"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase.\r\n",
        "\r\n",
        "When using packed padded sequences, we need to tell PyTorch how long the actual (non-padded) sequences are. Luckily for us, TorchText's `Field` objects allow us to use the `include_lengths` argument, this will cause our `batch.src` to be a tuple. The first element of the tuple is the same as before, a batch of numericalized source sentence as a tensor, and the second element is the non-padded lengths of each source sentence within the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPLXQgHHf1jN"
      },
      "source": [
        "SRC= data.Field(sequential = True, tokenize = 'spacy',init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True,include_lengths=True)\r\n",
        "TRG = data.Field(sequential = True, tokenize = 'spacy',init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oFGBpLD-SWN"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRu2G06Uu9NT"
      },
      "source": [
        "fields = [('Question', SRC),('Answer',TRG)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456Hn8XE-f0Y"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-TlfW5lvDMP"
      },
      "source": [
        "example = [data.Example.fromlist([df.Question.iloc[i],df.Answer.iloc[i]], fields) for i in range(df.shape[0])] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBSbtg--kLP"
      },
      "source": [
        "Creating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgRNKj2FvNq0"
      },
      "source": [
        "QnADataset = data.Dataset(example, fields)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Dba1Gi-woo"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7F8QEr0hPI"
      },
      "source": [
        "(train_data, valid_data,test_data) = QnADataset.split(split_ratio=[0.80, 0.10,0.10], random_state=random.seed(SEED))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPnEo75S0tt4",
        "outputId": "43ed55da-764d-49ce-d6b5-dd293430b550"
      },
      "source": [
        "(len(train_data), len(valid_data),len(test_data))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2735, 342, 342)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmFSDM4S07xn",
        "outputId": "ecf32b69-34e0-4237-f4d5-45e6d9773476"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Question': ['was', 'mission', 'bay', 'campus', 'opened', 'in', '2003'], 'Answer': ['yes']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxX5mgk2-8VB"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into <unk> tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpDtuN2p0-K-"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8EUkcBO--z-"
      },
      "source": [
        "Finally, define the device and create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-UvpKXg1AkT"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3dHji_rANfr"
      },
      "source": [
        "For packed padded sequences is that all elements in the batch need to be sorted by their non-padded lengths in descending order, i.e. the first sentence in the batch needs to be the longest. We use two arguments of the iterator to handle this, sort_within_batch which tells the iterator that the contents of the batch need to be sorted, and sort_key a function which tells the iterator how to sort the elements in the batch. Here, we sort by the length of the src sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A19rx2ur1DZ9"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_key=lambda x : len(x.Question),\r\n",
        "    sort_within_batch=True, \r\n",
        "    device = device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVKknZPIER7g"
      },
      "source": [
        "### Building the Seq2Seq Model\r\n",
        "\r\n",
        "\r\n",
        "This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of `enc_dim * 2` to `enc_dim * 2 if encoder_is_bidirectional else enc_dim`. \r\n",
        "\r\n",
        "This seq2seq encapsulator is similar to the last two. The only difference is that the `encoder` returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the decoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that `hidden` and `encoder_outputs` are passed to the decoder. \r\n",
        "\r\n",
        "Briefly going over all of the steps:\r\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\r\n",
        "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\r\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\r\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\r\n",
        "- we then decode within a loop:\r\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\r\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\r\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as **appropriate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYeZdiaO1O5x"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def create_mask(self, src):\r\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\r\n",
        "        return mask\r\n",
        "        \r\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        #src_len = [batch size]\r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\r\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\r\n",
        "                    \r\n",
        "        batch_size = src.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "        \r\n",
        "        #tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\r\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\r\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\r\n",
        "                \r\n",
        "        #first input to the decoder is the <sos> tokens\r\n",
        "        input = trg[0,:]\r\n",
        "        \r\n",
        "        mask = self.create_mask(src)\r\n",
        "\r\n",
        "        #mask = [batch size, src len]\r\n",
        "                \r\n",
        "        for t in range(1, trg_len):\r\n",
        "            \r\n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \r\n",
        "            #  and mask\r\n",
        "            #receive output tensor (predictions) and new hidden state\r\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\r\n",
        "            \r\n",
        "            #place predictions in a tensor holding predictions for each token\r\n",
        "            outputs[t] = output\r\n",
        "            \r\n",
        "            #decide if we are going to use teacher forcing or not\r\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "            \r\n",
        "            #get the highest predicted token from our predictions\r\n",
        "            top1 = output.argmax(1) \r\n",
        "            \r\n",
        "            #if teacher forcing, use actual next token as next input\r\n",
        "            #if not, use predicted token\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "            \r\n",
        "        return outputs"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOk39y21_C_w"
      },
      "source": [
        "\r\n",
        "### Encoder\r\n",
        "\r\n",
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. \r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq8.png?raw=1)\r\n",
        "\r\n",
        "We now have:\r\n",
        "\r\n",
        "$$\\begin{align*}\r\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\r\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\r\n",
        "\\end{align*}$$\r\n",
        "\r\n",
        "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\r\n",
        "\r\n",
        "As before, we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\r\n",
        "\r\n",
        "The RNN returns `outputs` and `hidden`. \r\n",
        "\r\n",
        "`outputs` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\r\n",
        "\r\n",
        "`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\r\n",
        "\r\n",
        "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \r\n",
        "\r\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\r\n",
        "\r\n",
        "**Note**: this is actually a deviation from the paper. Instead, they feed only the first backward RNN hidden state through a linear layer to get the context vector/decoder initial hidden state. This doesn't seem to make sense to me, so we have changed it.\r\n",
        "\r\n",
        "As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder.\r\n",
        "\r\n",
        "\r\n",
        "After the source sentence (padded automatically within the iterator) has been embedded, we can then use pack_padded_sequence on it with the lengths of the sentences. packed_embedded will then be our packed padded sequence. This can be then fed to our RNN as normal which will return packed_outputs, a packed tensor containing all of the hidden states from the sequence, and hidden which is simply the final hidden state from our sequence. hidden is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final non-padded element in the sequence.\r\n",
        "\r\n",
        "We then unpack our packed_outputs using pad_packed_sequence which returns the outputs and the lengths of each, which we don't need.\r\n",
        "\r\n",
        "The first dimension of outputs is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMS-S2L61IHE"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_len):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        #src_len = [batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(src))\r\n",
        "        \r\n",
        "        #embedded = [src len, batch size, emb dim]\r\n",
        "                \r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\r\n",
        "                \r\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\r\n",
        "                                 \r\n",
        "        #packed_outputs is a packed sequence containing all hidden states\r\n",
        "        #hidden is now from the final non-padded element in the batch\r\n",
        "            \r\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \r\n",
        "            \r\n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\r\n",
        "        #  when the input is a pad token are all zeros\r\n",
        "            \r\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\r\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\r\n",
        "        #outputs are always from the last layer\r\n",
        "        \r\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \r\n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\r\n",
        "        \r\n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \r\n",
        "        #  encoder RNNs fed through a linear layer\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\r\n",
        "        \r\n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        \r\n",
        "        return outputs, hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiPrXt98_Kqy"
      },
      "source": [
        "### Attention\r\n",
        "\r\n",
        "Next up is the attention layer. The attention module is where we calculate the attention values over the source sentence.  This will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\r\n",
        "\r\n",
        "Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$. \r\n",
        "\r\n",
        "\r\n",
        "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/teal blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq9.png?raw=1)\r\n",
        "\r\n",
        "\r\n",
        "Previously, we allowed this module to \"pay attention\" to padding tokens within the source sentence. However, using *masking*, we can force the attention to only be over non-padding elements.\r\n",
        "\r\n",
        "The `forward` method now takes a `mask` input. This is a **[batch size, source sentence length]** tensor that is 1 when the source sentence token is not a padding token, and 0 when it is a padding token. For example, if the source sentence is: [\"hello\", \"how\", \"are\", \"you\", \"?\", `<pad>`, `<pad>`], then the mask would be [1, 1, 1, 1, 1, 0, 0].\r\n",
        "\r\n",
        "We apply the mask after the attention has been calculated, but before it has been normalized by the `softmax` function. It is applied using `masked_fill`. This fills the tensor at each element where the first argument (`mask == 0`) is true, with the value given by the second argument (`-1e10`). In other words, it will take the un-normalized attention values, and change the attention values over padded elements to be `-1e10`. As these numbers will be miniscule compared to the other values they will become zero when passed through the `softmax` layer, ensuring no attention is payed to padding tokens in the source sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suh4d6hiEG7H"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\r\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\r\n",
        "        \r\n",
        "    def forward(self, hidden, encoder_outputs, mask):\r\n",
        "        \r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "        \r\n",
        "        #repeat decoder hidden state src_len times\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "  \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #hidden = [batch size, src len, dec hid dim]\r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \r\n",
        "        \r\n",
        "        #energy = [batch size, src len, dec hid dim]\r\n",
        "\r\n",
        "        attention = self.v(energy).squeeze(2)\r\n",
        "        \r\n",
        "        #attention = [batch size, src len]\r\n",
        "        \r\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDA03_cM_QVp"
      },
      "source": [
        "### Decoder\r\n",
        "\r\n",
        "Next up is the decoder. \r\n",
        "\r\n",
        "The decoder contains the attention layer, `attention`, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\r\n",
        "\r\n",
        "We then use this attention vector to create a weighted source vector, $w_t$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\r\n",
        "\r\n",
        "$$w_t = a_t H$$\r\n",
        "\r\n",
        "The embedded input word, $d(y_t)$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $d(y_t)$ and $w_t$ being concatenated together.\r\n",
        "\r\n",
        "$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\r\n",
        "\r\n",
        "We then pass $d(y_t)$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\r\n",
        "\r\n",
        "$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\r\n",
        "\r\n",
        "The image below shows decoding the first word in an example translation.\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq10.png?raw=1)\r\n",
        "\r\n",
        "The green/teal blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spy0krTg1NUo"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.attention = attention\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\r\n",
        "             \r\n",
        "        #input = [batch size]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        #mask = [batch size, src len]\r\n",
        "        \r\n",
        "        input = input.unsqueeze(0)\r\n",
        "        \r\n",
        "        #input = [1, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(input))\r\n",
        "        \r\n",
        "        #embedded = [1, batch size, emb dim]\r\n",
        "        \r\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\r\n",
        "                \r\n",
        "        #a = [batch size, src len]\r\n",
        "        \r\n",
        "        a = a.unsqueeze(1)\r\n",
        "        \r\n",
        "        #a = [batch size, 1, src len]\r\n",
        "        \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = torch.bmm(a, encoder_outputs)\r\n",
        "        \r\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = weighted.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\r\n",
        "        \r\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\r\n",
        "            \r\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n",
        "        \r\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\r\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\r\n",
        "        \r\n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\r\n",
        "        #output = [1, batch size, dec hid dim]\r\n",
        "        #hidden = [1, batch size, dec hid dim]\r\n",
        "        #this also means that output == hidden\r\n",
        "        assert (output == hidden).all()\r\n",
        "        \r\n",
        "        embedded = embedded.squeeze(0)\r\n",
        "        output = output.squeeze(0)\r\n",
        "        weighted = weighted.squeeze(0)\r\n",
        "        \r\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\r\n",
        "        \r\n",
        "        #prediction = [batch size, output dim]\r\n",
        "        \r\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMHSG524_WCr"
      },
      "source": [
        "# Training the Seq2Seq Model\r\n",
        "\r\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). The embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftEpd_4a1QmF"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "ENC_EMB_DIM = 256\r\n",
        "DEC_EMB_DIM = 256\r\n",
        "ENC_HID_DIM = 512\r\n",
        "DEC_HID_DIM = 512\r\n",
        "ENC_DROPOUT = 0.5\r\n",
        "DEC_DROPOUT = 0.5\r\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "\r\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8nTb43V_Zyo"
      },
      "source": [
        "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from  N(0,0.01) ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMp_2W9S1SVR",
        "outputId": "6f9ef6d0-d4a9-4a68-8892-6c5ac963283c"
      },
      "source": [
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        if 'weight' in name:\r\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        else:\r\n",
        "            nn.init.constant_(param.data, 0)\r\n",
        "            \r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(2197, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(1504, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=1504, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5_2i7Dk_fdz"
      },
      "source": [
        "We print out the number of parameters.\r\n",
        "\r\n",
        "Calculate the number of parameters. We get an increase of almost 50% in the amount of parameters from the last model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enkgnath1Tt3",
        "outputId": "e160f7fb-ff46-41d2-c971-37479c209139"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 10,077,408 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH3WH5Lh_mPO"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUoyH79C1W7L"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khbcq7R4_p-w"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFiHkSJC1YWm"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tuZ-o3l_xKO"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GeuBp3t1ZiB"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src, src_len = batch.Question\r\n",
        "        src_len=src_len.cpu()\r\n",
        "        trg = batch.Answer\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output = model(src, src_len, trg)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMTod0P_2I9"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to eval mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3z3OEyq1byl"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src, src_len = batch.Question\r\n",
        "            src_len=src_len.cpu()\r\n",
        "\r\n",
        "            trg = batch.Answer\r\n",
        "\r\n",
        "            output = model(src, src_len, trg, 0) #turn off teacher forcing\r\n",
        "            \r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjycjEXz_6h4"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkH2vb6z1drm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuTL4wYQ_8-W"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGldAX9N1e_8",
        "outputId": "19a6dbd1-6ad2-41c0-c88d-83b0082484af"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 7s\n",
            "\tTrain Loss: 5.200 | Train PPL: 181.184\n",
            "\t Val. Loss: 3.921 |  Val. PPL:  50.440\n",
            "Epoch: 02 | Time: 0m 7s\n",
            "\tTrain Loss: 4.331 | Train PPL:  75.987\n",
            "\t Val. Loss: 3.726 |  Val. PPL:  41.507\n",
            "Epoch: 03 | Time: 0m 7s\n",
            "\tTrain Loss: 4.099 | Train PPL:  60.276\n",
            "\t Val. Loss: 3.727 |  Val. PPL:  41.558\n",
            "Epoch: 04 | Time: 0m 7s\n",
            "\tTrain Loss: 3.887 | Train PPL:  48.765\n",
            "\t Val. Loss: 3.663 |  Val. PPL:  38.987\n",
            "Epoch: 05 | Time: 0m 7s\n",
            "\tTrain Loss: 3.671 | Train PPL:  39.278\n",
            "\t Val. Loss: 3.580 |  Val. PPL:  35.877\n",
            "Epoch: 06 | Time: 0m 7s\n",
            "\tTrain Loss: 3.413 | Train PPL:  30.355\n",
            "\t Val. Loss: 3.509 |  Val. PPL:  33.425\n",
            "Epoch: 07 | Time: 0m 7s\n",
            "\tTrain Loss: 3.171 | Train PPL:  23.834\n",
            "\t Val. Loss: 3.536 |  Val. PPL:  34.336\n",
            "Epoch: 08 | Time: 0m 7s\n",
            "\tTrain Loss: 2.901 | Train PPL:  18.193\n",
            "\t Val. Loss: 3.524 |  Val. PPL:  33.914\n",
            "Epoch: 09 | Time: 0m 7s\n",
            "\tTrain Loss: 2.619 | Train PPL:  13.722\n",
            "\t Val. Loss: 3.506 |  Val. PPL:  33.320\n",
            "Epoch: 10 | Time: 0m 7s\n",
            "\tTrain Loss: 2.333 | Train PPL:  10.309\n",
            "\t Val. Loss: 3.523 |  Val. PPL:  33.884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTCrvQ3-ADUe"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wODowUY1hHo",
        "outputId": "8d1c4170-9995-4639-980e-a3a9f7548e1f"
      },
      "source": [
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.399 | Test PPL:  29.946 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kZXSc5SAGiG"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtCVL3y_FRg9"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('de')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "    \r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\r\n",
        "    src_len=src_len.cpu()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\r\n",
        "\r\n",
        "    mask = model.create_mask(src_tensor)\r\n",
        "        \r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\r\n",
        "    \r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\r\n",
        "                \r\n",
        "        with torch.no_grad():\r\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\r\n",
        "\r\n",
        "        attentions[i] = attention\r\n",
        "            \r\n",
        "        pred_token = output.argmax(1).item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0dQmNUbhpUS"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize=(10,10))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "    \r\n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\r\n",
        "    \r\n",
        "    cax = ax.matshow(attention, cmap='bone')\r\n",
        "   \r\n",
        "    ax.tick_params(labelsize=15)\r\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n",
        "                       rotation=45)\r\n",
        "    ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKuzxxT1hsly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed00bc0c-4b8d-4a04-d02a-2a7cb0954d84"
      },
      "source": [
        "example_idx = 12\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['Question']\r\n",
        "trg = vars(train_data.examples[example_idx])['Answer']\r\n",
        "\r\n",
        "print(f'question = {src}')\r\n",
        "print(f'answer = {trg}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question = ['(', 'where', 'french', 'has', 'a', 'similar', 'phenomenon', ',', 'with', 'alveolar', 'affricates', 'instead', 'of', 'postalveolars', '?']\n",
            "answer = ['quebec', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gt7xxzEiHvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac88201-97ce-4093-fd8d-0eaf3ed08d90"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['nikola', 'tesla', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9kVdU2RiLIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1726ceb7-5971-421e-f5d0-30db9aad6da4"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_E4upCiLxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bb64f8-e15c-4695-f0c0-152c4f5f8f6e"
      },
      "source": [
        "example_idx = 14\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['Question']\r\n",
        "trg = vars(train_data.examples[example_idx])['Answer']\r\n",
        "\r\n",
        "print(f'question = {src}')\r\n",
        "print(f'answer = {trg}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question = ['where', 'do', 'sea', 'otters', 'live', '?']\n",
            "answer = ['sea', 'otters', '(', 'enhydra', 'lutris', ')', 'live', 'along', 'the', 'pacific', 'coast', 'of', 'north', 'america', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_obRBu8iPQK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "d9bb55c1-cee3-4702-e848-ee5b3a990bf8"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')\r\n",
        "\r\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['in', 'dense', 'vegetation', 'lives', 'weight', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kGhz_3iRTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726c94d4-5df8-40c0-8bd7-20648c9989ad"
      },
      "source": [
        "example_idx = 18\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['Question']\r\n",
        "trg = vars(train_data.examples[example_idx])['Answer']\r\n",
        "\r\n",
        "print(f'question = {src}')\r\n",
        "print(f'answer = {trg}')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question = ['is', 'it', 'true', 'that', 'he', 'published', 'a', 'collection', 'in', '1738', '?']\n",
            "answer = ['no']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tN01BGYiV87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "06b53a9f-a3bb-4a90-e866-b3be7b4d9683"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')\r\n",
        "\r\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['yes', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmBRqfzABB-f"
      },
      "source": [
        "## BLEU\r\n",
        "\r\n",
        "Previously we have only cared about the loss/perplexity of the model. However there metrics that are specifically designed for measuring the quality of a translation - the most popular is *BLEU*. Without going into too much detail, BLEU looks at the overlap in the predicted and actual target sequences in terms of their n-grams. It will give us a number between 0 and 1 for each sequence, where 1 means there is perfect overlap, i.e. a perfect translation, although is usually shown between 0 and 100. BLEU was designed for multiple candidate translations per source sequence, however in this dataset we only have one candidate per source.\r\n",
        "\r\n",
        "We define a `calculate_bleu` function which calculates the BLEU score over a provided TorchText dataset. This function creates a corpus of the actual and predicted translation for each source sentence and then calculates the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV5bmLQdBCbj"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\r\n",
        "\r\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\r\n",
        "    \r\n",
        "    trgs = []\r\n",
        "    pred_trgs = []\r\n",
        "    \r\n",
        "    for datum in data:\r\n",
        "        \r\n",
        "        src = vars(datum)['Question']\r\n",
        "        trg = vars(datum)['Answer']\r\n",
        "        \r\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\r\n",
        "        \r\n",
        "        #cut off <eos> token\r\n",
        "        pred_trg = pred_trg[:-1]\r\n",
        "        \r\n",
        "        pred_trgs.append(pred_trg)\r\n",
        "        trgs.append([trg])\r\n",
        "        \r\n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woNYNMgwBJZE",
        "outputId": "24672e87-5c6f-4087-8e33-25e819a81f50"
      },
      "source": [
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu9rp7zNEcRw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}